# Assignment 1

## Part 1

Implemented the following:

- Linear Layers
- MLP (Multi Layer Perceptron) models
- Activation functions: ReLU, Sigmoid, Tanh
- Loss functions: CrossEntropyLoss, MSELoss
- Optimizers: SGD, Adam
- Normalization: BatchNorm1d

## Part 2

Link to the [Kaggle Competition](https://www.kaggle.com/competitions/11785-hw1p2-f24/leaderboard?search=Yash+Thakkar)

Developed a speech recognition system for phoneme classification. Implemented a multi-layer perceptron neural network from scratch using PyTorch to process mel-spectrogram frames and classify them into phoneme states. Overall, the project involved:

- Architecting and optimizing an MLP model with custom hyperparameters
- Implementing comprehensive model training and evaluation pipelines with proper validation metrics
- Exploring various model configurations to maximize accuracy (layer depth, activation functions, learning rate schedulers, etc.)
- Applying advanced regularization techniques such as dropout and batch normalization
- Achieving significant improvement over baseline model accuracy through systematic hyperparameter tuning
- Participating in a Kaggle competition to benchmark model performance

The implementation has:

- Feed Forward Neural Network
- GELU activation function
- Batch Normalization and Dropout
- AdamW optimizer
- Cosine Annealing with Warm Restart learning rate scheduler
