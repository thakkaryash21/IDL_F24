# Assignment 1

## Part 1

Implemented the following:

- Linear Layers
- MLP (Multi Layer Perceptron) models
- Activation functions: ReLU, Sigmoid, Tanh
- Loss functions: CrossEntropyLoss, MSELoss
- Optimizers: SGD, Adam
- Normalization: BatchNorm1d

## Part 2

Link to the [Kaggle Competition](https://www.kaggle.com/competitions/11785-hw1p2-f24/leaderboard?search=Yash+Thakkar)

The competition focuses on phoneme recognition from MFCC features. The implementation of the code has:
- Feed Forward Neural Network
- GELU activation function
- Batch Normalization and Dropout
- AdamW optimizer
- Cosine Annealing with Warm Restart learning rate scheduler
